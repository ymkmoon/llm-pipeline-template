## í„°ë¯¸ë„ ì±„íŒ…
## íŒŒì´ì¬ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ ë™ì•ˆ ëŒ€í™” ë‚´ìš©ì´ ë©”ëª¨ë¦¬ì— ìœ ì§€ ë¨
## python rag_chat_v2.py

# rag_chat_v2.py
import os
import textwrap
from dotenv import load_dotenv
from langchain.chains import RetrievalQA
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_openai import ChatOpenAI

# ----------------------------
# í™˜ê²½ì„¤ì •
# ----------------------------
load_dotenv()
DB_PATH = os.getenv("EMBEDDING_DB_PATH", "./data_collection_db")
LM_STUDIO_URL = os.getenv("LM_STUDIO_URL", "http://localhost:1234/v1")
LM_MODEL = os.getenv("LM_MODEL", "openai/gpt-oss-20b")

os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["OMP_NUM_THREADS"] = "1"

# ----------------------------
# ì„ë² ë”© + ë²¡í„°DB
# ----------------------------
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
)
db = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)
retriever = db.as_retriever(search_type="mmr", search_kwargs={"k": 2})

# ----------------------------
# LLM ì—°ê²° (LM Studio)
# ----------------------------
llm = ChatOpenAI(
    model=LM_MODEL,
    openai_api_key="not-needed",
    openai_api_base=LM_STUDIO_URL,
    temperature=0.3,
)

# ----------------------------
# ëŒ€í™” ìš”ì•½ + ê¸°ì–µ
# ----------------------------
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)

def summarize_text(text: str, max_length: int = 2000):
    """ë¬¸ì„œê°€ ë„ˆë¬´ ê¸¸ë©´ ìë™ ìš”ì•½"""
    if len(text) <= max_length:
        return text
    chunks = textwrap.wrap(text, max_length)
    summaries = []
    for i, chunk in enumerate(chunks):
        print(f"  â”” ë¬¸ì„œ ìš”ì•½ ì¤‘ ({i+1}/{len(chunks)}) ...")
        prompt = (
            f"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”. í•µì‹¬ ì •ë³´ëŠ” ìœ ì§€í•˜ì„¸ìš”.\n\n{chunk}"
        )
        summary = llm.invoke(prompt).content
        summaries.append(summary)
    return "\n".join(summaries)

# ----------------------------
# ëŒ€í™”í˜• RAG í”„ë¡¬í”„íŠ¸
# ----------------------------
RAG_PROMPT = PromptTemplate(
    input_variables=["question", "context", "chat_history"],
    template=(
        "ë‹¹ì‹ ì€ ì§€ì ì´ê³  ë”°ëœ»í•œ í•œêµ­ì–´ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n"
        "ë‹¤ìŒì€ ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™” ë‚´ìš©ì…ë‹ˆë‹¤:\n"
        "{chat_history}\n\n"
        "ì•„ë˜ì˜ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•˜ì„¸ìš”.\n"
        "ë§Œì•½ ë¬¸ì„œì— ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ ìƒì‹ì ì¸ ëŒ€í™”ë¡œ ì‘ë‹µí•˜ì„¸ìš”.\n\n"
        "ë¬¸ì„œì˜ ì‘ì„±ìëŠ” ëª¨ë‘ ìœ ëª…ê¸° ì…ë‹ˆë‹¤.\n"
        "ìœ ëª…ê¸°ëŠ” ì£¼ë¡œ ymkmoon ë¼ëŠ” ë‹‰ë„¤ì„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n"
        "ì§ˆë¬¸: {question}\n\n"
        "ê´€ë ¨ ë¬¸ì„œ ë‚´ìš©:\n{context}\n\n"
        "ğŸ§  ë‹µë³€:"
    )
)

# ----------------------------
# ëŒ€í™” ê¸°ì–µí˜• RAG ì²´ì¸
# ----------------------------
def rag_with_memory(query: str):
    # ë¬¸ì„œ ê²€ìƒ‰
    docs = retriever.invoke(query)
    print(f"ğŸ“„ ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(docs)}")

    if len(docs) == 0:
        print("âš ï¸ ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ â†’ ì¼ë°˜ ëŒ€í™” ëª¨ë“œë¡œ ì „í™˜")
        response = llm.invoke(f"ì§ˆë¬¸: {query}\nìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•´ì¤˜.")
        memory.chat_memory.add_user_message(query)
        memory.chat_memory.add_ai_message(response.content)
        return response.content

    # ë¬¸ì„œ ìš”ì•½
    summarized_docs = []
    for i, doc in enumerate(docs):
        print(f"  â”œ ë¬¸ì„œ {i+1} ê¸¸ì´: {len(doc.page_content)}ì")
        if len(doc.page_content) > 1500:
            print(f"  â”” ë¬¸ì„œ {i+1} ìš”ì•½ ì¤‘ ...")
            doc.page_content = summarize_text(doc.page_content)
        summarized_docs.append(doc.page_content)

    context = "\n\n".join(summarized_docs)
    chat_history = "\n".join(
        [f"ì‚¬ìš©ì: {m.content}" if m.type == "human" else f"AI: {m.content}" for m in memory.chat_memory.messages]
    )

    final_prompt = RAG_PROMPT.format(question=query, context=context, chat_history=chat_history)

    # ë‹µë³€ ìƒì„±
    try:
        response = llm.invoke(final_prompt)
        memory.chat_memory.add_user_message(query)
        memory.chat_memory.add_ai_message(response.content)
        return response.content.strip()
    except Exception as e:
        print(f"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
        response = llm.invoke(f"ì§ˆë¬¸: {query}\nê°„ë‹¨íˆ ë‹µë³€í•´ì¤˜.")
        return response.content.strip()

# ----------------------------
# í„°ë¯¸ë„ ì‹¤í–‰ ë£¨í”„
# ----------------------------
def main():
    print("ğŸ’¬ ëŒ€í™” ê¸°ì–µí˜• RAG Chat ì‹œì‘")
    print("ì¢…ë£Œí•˜ë ¤ë©´ 'exit', 'quit', 'ì¢…ë£Œ' ì…ë ¥")

    while True:
        try:
            query = input("\nì§ˆë¬¸: ").strip()
            if query.lower() in ["exit", "quit", "ì¢…ë£Œ"]:
                print("ğŸ‘‹ ì±„íŒ…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            if not query:
                print("âš ï¸ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                continue

            answer = rag_with_memory(query)
            print(f"\nğŸ§  ë‹µë³€: {answer}\n")

        except KeyboardInterrupt:
            print("\nğŸ‘‹ ì±„íŒ…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤ (Ctrl+C).")
            break
        except Exception as e:
            print(f"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {e}")

# ----------------------------
if __name__ == "__main__":
    main()
