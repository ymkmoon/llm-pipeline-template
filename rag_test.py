## LLM íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì½”ë“œ
## python rag_test.py

# rag_test.py
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv

# ====== .env ë¡œë“œ ======
load_dotenv()

# ====== í™˜ê²½ ë³€ìˆ˜ ======
DB_PATH = os.getenv("EMBEDDING_DB_PATH")

# 1) LLM (LM Studio API ì„œë²„ ì—°ê²°)
llm = ChatOpenAI(
    model="openai/gpt-oss-20b",    # âœ… LM Studioì—ì„œ ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ ì´ë¦„
    openai_api_key="not-needed",   # LM StudioëŠ” API key ë¶ˆí•„ìš”
    openai_api_base="http://localhost:1234/v1"
)

# 2) ë²¡í„°DB ë¡œë“œ (ğŸ‘‰ MiniLMìœ¼ë¡œ í†µì¼)
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
)
db = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)
retriever = db.as_retriever()

# 3) QA ì²´ì¸ ìƒì„±
qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)

# 4) í…ŒìŠ¤íŠ¸ ì§ˆë¬¸
query = "Java Stream APIë¥¼ ì–´ë–»ê²Œ í™œìš©í•  ìˆ˜ ìˆì–´?"
print("Q:", query)
# run() ëŒ€ì‹  invoke() ê¶Œì¥
result = qa.invoke({"query": query})
print("A:", result["result"])
