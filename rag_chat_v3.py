## í„°ë¯¸ë„ ì±„íŒ…
## ëŒ€í™” ë‚´ìš©ì„ ì˜êµ¬ ë³´ê´€
## python rag_chat_v3.py

# rag_chat_v3.py
import os
import json
import textwrap
from dotenv import load_dotenv
from langchain.chains import RetrievalQA
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_openai import ChatOpenAI

# ----------------------------
# í™˜ê²½ì„¤ì •
# ----------------------------
load_dotenv()
DB_PATH = os.getenv("EMBEDDING_DB_PATH", "./data_collection_db")
LM_STUDIO_URL = os.getenv("LM_STUDIO_URL", "http://localhost:1234/v1")
LM_MODEL = os.getenv("LM_MODEL", "openai/gpt-oss-20b")

os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["OMP_NUM_THREADS"] = "1"

# ----------------------------
# ë‹‰ë„¤ì„ ì…ë ¥
# ----------------------------
def get_nickname():
    while True:
        nickname = input("ğŸ‘¤ ë‹‰ë„¤ì„ (5ê¸€ì) ì…ë ¥: ").strip()
        if len(nickname) == 5:
            return nickname
        if nickname == "ymkmoon":
            return nickname
        print("âš ï¸ ë‹‰ë„¤ì„ì€ ì •í™•íˆ 5ê¸€ìì—¬ì•¼ í•©ë‹ˆë‹¤.")

nickname = get_nickname()
CHAT_HISTORY_PATH = f"./chat_history_{nickname}.json"

# ----------------------------
# ì„ë² ë”© + ë²¡í„°DB
# ----------------------------
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
)
db = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)
retriever = db.as_retriever(search_type="mmr", search_kwargs={"k": 5})

# ----------------------------
# LLM ì—°ê²° (LM Studio)
# ----------------------------
llm = ChatOpenAI(
    model=LM_MODEL,
    openai_api_key="not-needed",
    openai_api_base=LM_STUDIO_URL,
    temperature=0.3,
)

# ----------------------------
# ëŒ€í™” ê¸°ì–µ
# ----------------------------
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)

# ì´ì „ chat_history ë³µì›
if os.path.exists(CHAT_HISTORY_PATH):
    try:
        with open(CHAT_HISTORY_PATH, "r", encoding="utf-8") as f:
            messages = json.load(f)
            for msg in messages:
                if msg['type'] == 'human':
                    memory.chat_memory.add_user_message(msg['content'])
                else:
                    memory.chat_memory.add_ai_message(msg['content'])
        print(f"âœ… ì´ì „ ëŒ€í™” {len(messages)}ê±´ì„ ë³µì›í–ˆìŠµë‹ˆë‹¤. (ë‹‰ë„¤ì„: {nickname})")
    except Exception as e:
        print(f"âš ï¸ chat_history ë³µì› ì‹¤íŒ¨: {e}")


def save_chat_history():
    try:
        with open(CHAT_HISTORY_PATH, "w", encoding="utf-8") as f:
            json.dump([m.dict() for m in memory.chat_memory.messages], f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ chat_history ì €ì¥ ì™„ë£Œ ({len(memory.chat_memory.messages)}ê±´)")
    except Exception as e:
        print(f"âš ï¸ chat_history ì €ì¥ ì‹¤íŒ¨: {e}")

# ----------------------------
# ë¬¸ì„œ ìš”ì•½
# ----------------------------
import itertools
import sys
import threading
import time

def summarize_text(text: str, max_length: int = 2000):
    """ë¬¸ì„œê°€ ë„ˆë¬´ ê¸¸ë©´ ìë™ ìš”ì•½ + ì§„í–‰ ì¤‘ í‘œì‹œ"""
    if len(text) <= max_length:
        return text

    chunks = textwrap.wrap(text, max_length)
    summaries = []
    total = len(chunks)

    # ìŠ¤í”¼ë„ˆ ì“°ë ˆë“œ
    stop_spinner = False
    def spinner():
        for c in itertools.cycle("|/-\\"):
            if stop_spinner:
                break
            print(f"\r  â”” ë¬¸ì„œ ìš”ì•½ ì¤‘ ({i}/{total}) ... {c}", end="", flush=True)
            time.sleep(0.1)

    for i, chunk in enumerate(chunks, start=1):
        stop_spinner = False
        t = threading.Thread(target=spinner)
        t.start()

        # ìš”ì•½ ìƒì„±
        prompt = (
            f"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”. í•µì‹¬ ì •ë³´ëŠ” ìœ ì§€í•˜ì„¸ìš”.\n\n{chunk}"
        )
        summary = llm.invoke(prompt).content
        summaries.append(summary)

        # ìŠ¤í”¼ë„ˆ ì¢…ë£Œ
        stop_spinner = True
        t.join()
    
    # ì™„ë£Œ ë©”ì‹œì§€ ì¶œë ¥
    print(f"\r  â”” ë¬¸ì„œ ìš”ì•½ ì™„ë£Œ ({total}/{total}) âœ…{' ' * 20}")  
    return "\n".join(summaries)



# ----------------------------
# RAG í”„ë¡¬í”„íŠ¸
# ----------------------------
RAG_PROMPT = PromptTemplate(
    input_variables=["question", "context", "chat_history"],
    template=(
        "ë‹¹ì‹ ì€ ì§€ì ì´ê³  ë”°ëœ»í•œ í•œêµ­ì–´ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n"
        "ë‹¤ìŒì€ ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™” ë‚´ìš©ì…ë‹ˆë‹¤:\n"
        "{chat_history}\n\n"
        "ì•„ë˜ì˜ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•˜ì„¸ìš”.\n"
        "ë¬¸ì„œì˜ ì‘ì„±ìëŠ” ëª¨ë‘ ìœ ëª…ê¸° ì…ë‹ˆë‹¤.\n"
        "ìœ ëª…ê¸°ëŠ” ì£¼ë¡œ ymkmoon ë¼ëŠ” ë‹‰ë„¤ì„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n"
        "ë§Œì•½ ë¬¸ì„œì— ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ ìƒì‹ì ì¸ ëŒ€í™”ë¡œ ì‘ë‹µí•˜ì„¸ìš”.\n\n"
        "ì§ˆë¬¸: {question}\n\n"
        "ê´€ë ¨ ë¬¸ì„œ ë‚´ìš©:\n{context}\n\n"
        "ğŸ§  ë‹µë³€:"
    )
)

# ----------------------------
# ê´€ë ¨ì„± ê¸°ë°˜ RAG ì²´ì¸ ìµœì í™”
# ----------------------------
RELEVANCE_THRESHOLD = 0.3  # ê´€ë ¨ì„± ì„ê³„ê°’

def rag_with_memory(query: str):
    # ----------------------------
    # ë‹‰ë„¤ì„ + ì§ˆë¬¸ ì¡°ê±´ í™•ì¸
    # ----------------------------
    if not (nickname.startswith("ymkmoon") and query.startswith("ymkmoon")):
        # print("âš ï¸ DB íƒìƒ‰ ì¡°ê±´ ë¯¸ì¶©ì¡± â†’ ì¼ë°˜ ëŒ€í™” ëª¨ë“œ")
        response = llm.invoke(f"ì§ˆë¬¸: {query}\nìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•´ì¤˜.")
        memory.chat_memory.add_user_message(query)
        memory.chat_memory.add_ai_message(response.content)
        save_chat_history()
        return response.content.strip()

    # ----------------------------
    # ì¡°ê±´ ì¶©ì¡± ì‹œ RAG ì‹¤í–‰
    # ----------------------------
    docs = retriever.invoke(query)
    print(f"ğŸ“„ ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(docs)}")

    # ê´€ë ¨ì„± ê¸°ë°˜ í•„í„°ë§ ë° ìš”ì•½
    context_docs = []
    for i, doc in enumerate(docs):
        relevance = getattr(doc, "score", 1.0)  # score ì†ì„±ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ 1.0
        print(f"  â”œ ë¬¸ì„œ {i+1} ê¸¸ì´: {len(doc.page_content)}ì, ê´€ë ¨ì„±: {relevance:.2f}")

        # ê´€ë ¨ì„± ë‚®ìœ¼ë©´ ì•„ì˜ˆ ì œì™¸
        if relevance < RELEVANCE_THRESHOLD:
            print(f"  â”” ë¬¸ì„œ {i+1} ê´€ë ¨ì„± ë‚®ìŒ â†’ contextì—ì„œ ì œì™¸")
            continue

        # ê¸¸ë©´ ìš”ì•½
        if len(doc.page_content) > 1500:
            doc.page_content = summarize_text(doc.page_content)

        context_docs.append(doc.page_content)

    # ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ â†’ ì¼ë°˜ ëŒ€í™” ëª¨ë“œ
    if len(context_docs) == 0:
        print("âš ï¸ ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ â†’ ì¼ë°˜ ëŒ€í™” ëª¨ë“œë¡œ ì „í™˜")
        response = llm.invoke(f"ì§ˆë¬¸: {query}\nìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•´ì¤˜.")
        memory.chat_memory.add_user_message(query)
        memory.chat_memory.add_ai_message(response.content)
        save_chat_history()
        return response.content.strip()

    # ----------------------------
    # context ìƒì„± ë° LLM í˜¸ì¶œ
    # ----------------------------
    context = "\n\n".join(context_docs)
    chat_history = "\n".join(
        [f"ì‚¬ìš©ì: {m.content}" if m.type == "human" else f"AI: {m.content}"
         for m in memory.chat_memory.messages]
    )

    final_prompt = RAG_PROMPT.format(
        question=query,
        context=context,
        chat_history=chat_history
    )

    try:
        response = llm.invoke(final_prompt)
        memory.chat_memory.add_user_message(query)
        memory.chat_memory.add_ai_message(response.content)
        save_chat_history()
        return response.content.strip()
    except Exception as e:
        print(f"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
        response = llm.invoke(f"ì§ˆë¬¸: {query}\nê°„ë‹¨íˆ ë‹µë³€í•´ì¤˜.")
        memory.chat_memory.add_user_message(query)
        memory.chat_memory.add_ai_message(response.content)
        save_chat_history()
        return response.content.strip()

# ----------------------------
# í„°ë¯¸ë„ ì‹¤í–‰ ë£¨í”„
# ----------------------------
def main():
    print(f"ğŸ’¬ ëŒ€í™” ê¸°ì–µí˜• RAG Chat ì‹œì‘ (ë‹‰ë„¤ì„: {nickname})")
    print("ì¢…ë£Œí•˜ë ¤ë©´ 'exit', 'quit', 'ì¢…ë£Œ' ì…ë ¥")

    while True:
        try:
            query = input("\nì§ˆë¬¸: ").strip()
            if query.lower() in ["exit", "quit", "ì¢…ë£Œ"]:
                print("ğŸ‘‹ ì±„íŒ…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                save_chat_history()
                break
            if not query:
                print("âš ï¸ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                continue

            answer = rag_with_memory(query)
            print(f"\nğŸ§  ë‹µë³€: {answer}\n")

        except KeyboardInterrupt:
            print("\nğŸ‘‹ ì±„íŒ…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤ (Ctrl+C).")
            save_chat_history()
            break
        except Exception as e:
            print(f"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {e}")

# ----------------------------
if __name__ == "__main__":
    main()
