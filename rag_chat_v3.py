## í„°ë¯¸ë„ ì±„íŒ…
## ëŒ€í™” ë‚´ìš©ì„ ì˜êµ¬ ë³´ê´€
## python rag_chat_v3.py
import os
import json
import textwrap
import itertools
import threading
import time
from dotenv import load_dotenv
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_openai import ChatOpenAI

# ----------------------------
# í™˜ê²½ì„¤ì •
# ----------------------------
load_dotenv()
DB_PATH = os.getenv("EMBEDDING_DB_PATH", "./data_collection_db")
LM_STUDIO_URL = os.getenv("LM_STUDIO_URL", "http://localhost:1234/v1")
LM_MODEL = os.getenv("LM_MODEL", "openai/gpt-oss-20b")
SEARCH_KWARGS = 10

os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["OMP_NUM_THREADS"] = "4"
os.environ["MKL_NUM_THREADS"] = "4"

# ----------------------------
# ë‹‰ë„¤ì„ ì…ë ¥
# ----------------------------
def get_nickname():
    while True:
        nickname = input("ğŸ‘¤ ë‹‰ë„¤ì„ (5ê¸€ì) ì…ë ¥: ").strip()
        if len(nickname) == 5 or nickname == "ymkmoon":
            return nickname
        print("âš ï¸ ë‹‰ë„¤ì„ì€ ì •í™•íˆ 5ê¸€ìì—¬ì•¼ í•©ë‹ˆë‹¤.")

nickname = get_nickname()
CHAT_HISTORY_PATH = f"./chat_history_{nickname}.json"

# ----------------------------
# ì„ë² ë”© + ë²¡í„°DB
# ----------------------------
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
)
db = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)
retriever = db.as_retriever(search_type="mmr", search_kwargs={"k": SEARCH_KWARGS})

# ----------------------------
# LLM ì—°ê²° (LM Studio)
# ----------------------------
llm = ChatOpenAI(
    model=LM_MODEL,
    openai_api_key="not-needed",
    openai_api_base=LM_STUDIO_URL,
    temperature=0.3,
)

# ----------------------------
# ëŒ€í™” ê¸°ì–µ
# ----------------------------
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)

# ì´ì „ chat_history ë³µì›
if os.path.exists(CHAT_HISTORY_PATH):
    try:
        with open(CHAT_HISTORY_PATH, "r", encoding="utf-8") as f:
            messages = json.load(f)
            for msg in messages:
                if msg['type'] == 'human':
                    memory.chat_memory.add_user_message(msg['content'])
                else:
                    memory.chat_memory.add_ai_message(msg['content'])
        print(f"âœ… ì´ì „ ëŒ€í™” {len(messages)}ê±´ì„ ë³µì›í–ˆìŠµë‹ˆë‹¤. (ë‹‰ë„¤ì„: {nickname})")
    except Exception as e:
        print(f"âš ï¸ chat_history ë³µì› ì‹¤íŒ¨: {e}")

# ----------------------------
# chat_history ì €ì¥
# ----------------------------
def save_chat_history():
    try:
        with open(CHAT_HISTORY_PATH, "w", encoding="utf-8") as f:
            json.dump([m.model_dump() for m in memory.chat_memory.messages], f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ chat_history ì €ì¥ ì™„ë£Œ ({len(memory.chat_memory.messages)}ê±´)")
    except Exception as e:
        print(f"âš ï¸ chat_history ì €ì¥ ì‹¤íŒ¨: {e}")

# ----------------------------
# ë¬¸ì„œ ìš”ì•½ + ì „ì²´ ì§„í–‰ë¥  í‘œì‹œ
# ----------------------------
def summarize_text(text: str, doc_index: int = 0, total_docs: int = 1, max_length: int = 2000):
    """ë¬¸ì„œê°€ ë„ˆë¬´ ê¸¸ë©´ ìë™ ìš”ì•½ + ì „ì²´ ì§„í–‰ë¥  í‘œì‹œ"""
    if len(text) <= max_length:
        return text

    chunks = textwrap.wrap(text, max_length)
    summaries = []
    total_chunks = len(chunks)

    for i, chunk in enumerate(chunks, start=1):
        stop_spinner = False

        def spinner():
            for c in itertools.cycle("|/-\\"):
                if stop_spinner:
                    break
                overall_progress = ((doc_index + i / total_chunks) / total_docs) * 100
                print(f"\r  â”” ì „ì²´ ë¬¸ì„œ ìš”ì•½ ì§„í–‰: {overall_progress:.1f}% ... {c}", end="", flush=True)
                time.sleep(0.1)

        t = threading.Thread(target=spinner)
        t.start()

        prompt = f"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”. í•µì‹¬ ì •ë³´ëŠ” ìœ ì§€í•˜ì„¸ìš”.\n\n{chunk}"
        summary = llm.invoke(prompt).content
        summaries.append(summary)

        stop_spinner = True
        t.join()

    return "\n".join(summaries)

# ----------------------------
# RAG í”„ë¡¬í”„íŠ¸
# ----------------------------
RAG_PROMPT = PromptTemplate(
    input_variables=["question", "context", "chat_history"],
    template=(
        "ë‹¹ì‹ ì€ ì§€ì ì´ê³  ë”°ëœ»í•œ í•œêµ­ì–´ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n"
        "ë‹¤ìŒì€ ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™” ë‚´ìš©ì…ë‹ˆë‹¤:\n"
        "{chat_history}\n\n"
        "ì•„ë˜ì˜ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•˜ì„¸ìš”.\n"
        "ë¬¸ì„œì˜ ì‘ì„±ìëŠ” ëª¨ë‘ ìœ ëª…ê¸° ì…ë‹ˆë‹¤.\n"
        "ìœ ëª…ê¸°ëŠ” ì£¼ë¡œ ymkmoon ë¼ëŠ” ë‹‰ë„¤ì„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n"
        "ë§Œì•½ ë¬¸ì„œì— ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ ìƒì‹ì ì¸ ëŒ€í™”ë¡œ ì‘ë‹µí•˜ì„¸ìš”.\n\n"
        "ì§ˆë¬¸: {question}\n\n"
        "ê´€ë ¨ ë¬¸ì„œ ë‚´ìš©:\n{context}\n\n"
        "ğŸ§  ë‹µë³€:"
    )
)

# ----------------------------
# ê´€ë ¨ì„± ê¸°ë°˜ RAG ì²´ì¸ ìµœì í™”
# ----------------------------
RELEVANCE_THRESHOLD = 0.3
MAX_CONTEXT_CHARS = 8000
MAX_CHAT_HISTORY_CHARS = 2000

def rag_with_memory(query: str):
    # ì¼ë°˜ ëŒ€í™” ëª¨ë“œ
    if not (nickname.startswith("ymkmoon") and query.startswith("ymkmoon")):
        response = llm.invoke(f"ì§ˆë¬¸: {query}\nìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•´ì¤˜.")
        memory.chat_memory.add_user_message(query)
        memory.chat_memory.add_ai_message(response.content)
        save_chat_history()
        return response.content.strip()

    # ----------------------------
    # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ë° ìš”ì•½
    # ----------------------------
    docs = retriever.invoke(query)
    print(f"ğŸ“„ ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(docs)}")
    context_docs = []

    valid_docs = [doc for doc in docs if len(doc.page_content.strip()) > 0 and getattr(doc, "score", 1.0) >= RELEVANCE_THRESHOLD]
    total_docs = len(valid_docs)

    for idx, doc in enumerate(valid_docs):
        if len(doc.page_content) > 1500:
            doc.page_content = summarize_text(doc.page_content, doc_index=idx, total_docs=total_docs)
        context_docs.append(doc.page_content)

    # context ìƒì„± + ê¸¸ì´ ì œí•œ
    context = "\n\n".join(context_docs)
    if len(context) > MAX_CONTEXT_CHARS:
        context = context[:MAX_CONTEXT_CHARS] + "\n... (ìƒëµ)"

    # ----------------------------
    # ìµœê·¼ ëŒ€í™” ì¶”ì¶œ + ê¸¸ì´ ì œí•œ
    # ----------------------------
    recent_msgs = reversed(memory.chat_memory.messages)
    chat_history = ""
    for m in recent_msgs:
        line = f"ì‚¬ìš©ì: {m.content}" if m.type=="human" else f"AI: {m.content}"
        if len(chat_history) + len(line) + 1 > MAX_CHAT_HISTORY_CHARS:
            break
        chat_history = line + "\n" + chat_history

    # ----------------------------
    # ìµœì¢… í”„ë¡¬í”„íŠ¸ ìƒì„±
    # ----------------------------
    final_prompt = RAG_PROMPT.format(
        question=query,
        context=context,
        chat_history=chat_history
    )

    # ----------------------------
    # LLM í˜¸ì¶œ + chat_history ì €ì¥
    # ----------------------------
    try:
        response = llm.invoke(final_prompt)
        memory.chat_memory.add_user_message(query)
        memory.chat_memory.add_ai_message(response.content)
        save_chat_history()
        return response.content.strip()
    except Exception as e:
        print(f"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
        response = llm.invoke(f"ì§ˆë¬¸: {query}\nê°„ë‹¨íˆ ë‹µë³€í•´ì¤˜.")
        memory.chat_memory.add_user_message(query)
        memory.chat_memory.add_ai_message(response.content)
        save_chat_history()
        return response.content.strip()

# ----------------------------
# í„°ë¯¸ë„ ì‹¤í–‰ ë£¨í”„
# ----------------------------
def main():
    print(f"ğŸ’¬ ëŒ€í™” ê¸°ì–µí˜• RAG Chat ì‹œì‘ (ë‹‰ë„¤ì„: {nickname})")
    print("ì¢…ë£Œí•˜ë ¤ë©´ 'exit', 'quit', 'ì¢…ë£Œ' ì…ë ¥")

    while True:
        try:
            query = input("\nì§ˆë¬¸: ").strip()
            if query.lower() in ["exit", "quit", "ì¢…ë£Œ"]:
                print("ğŸ‘‹ ì±„íŒ…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                save_chat_history()
                break
            if not query:
                print("âš ï¸ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                continue

            answer = rag_with_memory(query)
            print(f"\nğŸ§  ë‹µë³€: {answer}\n")

        except KeyboardInterrupt:
            print("\nğŸ‘‹ ì±„íŒ… ì¢…ë£Œ (Ctrl+C)")
            save_chat_history()
            break
        except Exception as e:
            print(f"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {e}")

# ----------------------------
if __name__ == "__main__":
    main()
