## í„°ë¯¸ë„ ì±„íŒ…
## python rag_chat_v1.py

# rag_chat_v1.py
import os
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain_chroma import Chroma
from dotenv import load_dotenv
import textwrap

# ----------------------------
# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
# ----------------------------
load_dotenv()
DB_PATH = os.getenv("EMBEDDING_DB_PATH", "./data_collection_db")
LM_STUDIO_URL = os.getenv("LM_STUDIO_URL", "http://localhost:1234/v1")
LM_MODEL = os.getenv("LM_MODEL", "openai/gpt-oss-20b")

# ë©€í‹°ìŠ¤ë ˆë“œ ë° í† í¬ë‚˜ì´ì € ê²½ê³  ë°©ì§€
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["OMP_NUM_THREADS"] = "1"

# ----------------------------
# ì„ë² ë”© & ë²¡í„° DB ë¡œë“œ
# ----------------------------
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
)
db = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)
retriever = db.as_retriever(search_type="mmr", search_kwargs={"k": 2})

# ----------------------------
# LLM (LM Studio ì—°ê²°)
# ----------------------------
llm = ChatOpenAI(
    model=LM_MODEL,
    openai_api_key="not-needed",
    openai_api_base=LM_STUDIO_URL,
    temperature=0.3,  # ì•½ê°„ì˜ ì°½ì˜ì„± ì¶”ê°€
)

# ----------------------------
# ê¸´ ë¬¸ì„œ ìš”ì•½ í•¨ìˆ˜
# ----------------------------
def summarize_text(text: str, max_length: int = 2000):
    """ë¬¸ì„œê°€ ë„ˆë¬´ ê¸¸ë©´ ìë™ìœ¼ë¡œ ìš”ì•½"""
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    if len(text) <= max_length:
        return text
    chunks = textwrap.wrap(text, max_length)
    summaries = []
    for i, chunk in enumerate(chunks):
        print(f"  â”” ë¬¸ì„œ ìš”ì•½ ì¤‘ ({i+1}/{len(chunks)}) ...")
        prompt = (
            f"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”. ì¤‘ìš” ì •ë³´ëŠ” ìœ ì§€í•´ì£¼ì„¸ìš”.\n\n{chunk}"
        )
        summary = llm.invoke(prompt).content
        summaries.append(summary)
    return "\n".join(summaries)

# ----------------------------
# RAGìš© í”„ë¡¬í”„íŠ¸ (ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”í˜•)
# ----------------------------
RAG_PROMPT = PromptTemplate(
    input_variables=["question", "context"],
    template=(
        "ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ì§€ì ì¸ í•œêµ­ì–´ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n"
        "ì•„ë˜ì˜ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€ë‹µí•˜ì„¸ìš”.\n"
        "ë§Œì•½ ë¬¸ì„œì— ë‹µì´ ì—†ìœ¼ë©´, ì¼ë°˜ì ì¸ ì§€ì‹ì´ë‚˜ ìƒì‹ ì„ ì—ì„œ ë‹µë³€í•˜ì„¸ìš”.\n\n"
        "ì§ˆë¬¸: {question}\n\n"
        "ê´€ë ¨ ë¬¸ì„œ ë‚´ìš©:\n{context}\n\n"
        "ğŸ§  ë‹µë³€:"
    ),
)

# ----------------------------
# RAG ì²´ì¸ (ìë™ ìš”ì•½ + Fallback í¬í•¨)
# ----------------------------
def rag_answer(query: str):
    # ë¬¸ì„œ ê²€ìƒ‰
    docs = retriever.invoke(query)
    print(f"ğŸ“„ ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(docs)}")
    if len(docs) == 0:
        print("âš ï¸ ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ â†’ ì¼ë°˜ ë‹µë³€ ëª¨ë“œë¡œ ì „í™˜")
        response = llm.invoke(f"ì§ˆë¬¸: {query}\nìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.")
        return response.content

    # ë¬¸ì„œ ê¸¸ì´ ì¶œë ¥ ë° ìš”ì•½ ì²˜ë¦¬
    summarized_docs = []
    for i, doc in enumerate(docs):
        print(f"  â”œ ë¬¸ì„œ {i+1} ê¸¸ì´: {len(doc.page_content)}ì")
        if len(doc.page_content) > 1500:
            print(f"  â”” ë¬¸ì„œ {i+1} ìš”ì•½ ì¤‘ ...")
            doc.page_content = summarize_text(doc.page_content)
        summarized_docs.append(doc.page_content)

    # ë¬¸ë§¥ ê²°í•©
    context = "\n\n".join(summarized_docs)

    # ìµœì¢… ì§ˆì˜ êµ¬ì„±
    final_prompt = RAG_PROMPT.format(question=query, context=context)

    # LLM í˜¸ì¶œ
    try:
        response = llm.invoke(final_prompt)
        return response.content.strip()
    except Exception as e:
        print(f"âš ï¸ LLM í˜¸ì¶œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        # Fallback: ë¬¸ë§¥ ì—†ì´ ì¼ë°˜ ë‹µë³€
        response = llm.invoke(f"ì§ˆë¬¸: {query}\nê°„ë‹¨íˆ ë‹µë³€í•´ì£¼ì„¸ìš”.")
        return response.content.strip()

# ----------------------------
# í„°ë¯¸ë„ ì±„íŒ… ë£¨í”„
# ----------------------------
def main():
    print("ğŸ’¬ RAG Chat ì‹œì‘")
    print("ì¢…ë£Œí•˜ë ¤ë©´ 'exit', 'quit', 'ì¢…ë£Œ' ì…ë ¥")

    while True:
        try:
            query = input("\nì§ˆë¬¸: ").strip()
            if query.lower() in ["exit", "quit", "ì¢…ë£Œ"]:
                print("ğŸ‘‹ ì±„íŒ…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            if not query:
                print("âš ï¸ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                continue

            answer = rag_answer(query)
            print(f"\nğŸ§  ë‹µë³€: {answer}\n")

        except KeyboardInterrupt:
            print("\nğŸ‘‹ ì±„íŒ…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤ (Ctrl+C).")
            break
        except Exception as e:
            print(f"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {e}")

# ----------------------------
if __name__ == "__main__":
    main()
