# rag_chat.py
import os
from langchain.chains import RetrievalQA
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain_chroma import Chroma

# í† í¬ë‚˜ì´ì € ë³‘ë ¬ ì²˜ë¦¬ ê²½ê³  ì œê±°
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# 1) LLM (LM Studio API ì„œë²„ ì—°ê²°)
llm = ChatOpenAI(
    model="openai/gpt-oss-20b",    # LM Studio ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ ì´ë¦„
    openai_api_key="not-needed",   # LM StudioëŠ” API key ë¶ˆí•„ìš”
    openai_api_base="http://localhost:1234/v1"
)

# 2) ë²¡í„°DB ë¡œë“œ (MiniLMìœ¼ë¡œ í†µì¼)
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
)
db = Chroma(persist_directory="./blog_db", embedding_function=embeddings)
retriever = db.as_retriever()

# 3) QA ì²´ì¸ ìƒì„±
qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)

# 4) í„°ë¯¸ë„ ì±„íŒ… ë£¨í”„
print("ğŸ’¬ RAG Chat ì‹œì‘ (ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ë˜ëŠ” 'quit' ì…ë ¥)")
while True:
    query = input("\nì§ˆë¬¸: ")
    if query.lower() in ["exit", "quit", "ì¢…ë£Œ"]:
        print("ğŸ‘‹ ì±„íŒ…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        break
    
    try:
        result = qa.invoke({"query": query})
        print("ë‹µë³€:", result["result"])
    except Exception as e:
        print("âš ï¸ ì˜¤ë¥˜ ë°œìƒ:", e)
